{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fastparquet import ParquetFile, write\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "import dask.dataframe as dd\n",
    "import holoviews as hv\n",
    "import datashader as ds\n",
    "import gc\n",
    "from os.path import join, dirname\n",
    "import pyarrow \n",
    "import pyarrow.parquet as pq\n",
    "# PLOT USING HOLOVIEWS DASK AND DATASHADER\n",
    "import hvplot.pandas\n",
    "import hvplot.dask\n",
    "import hvplot as hv\n",
    "from bokeh.models import HoverTool\n",
    "from pdb import set_trace\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "import dateutil.relativedelta\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UTILS \n",
    "import findspark\n",
    "findspark.init('/usr/local/spark/spark-2.3.2-bin-hadoop2.7')\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import DateType, StringType, IntegerType\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Poolminers\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc=SparkContext.getOrCreate(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "# EXPLODE THE TRANSACTION_LIST COLUMN IN AIONV4.BLOCK\n",
    "def explode_block(df1,col):\n",
    "    # explode the list the first time\n",
    "    df1 = df1.withColumn(col,explode(split(f.col(col),'\\],\\[') ))\n",
    "    # extract the transaction_hash\n",
    "    df1 = df1.withColumn(col,regexp_replace('transaction_list', '(\\[|\\]|\")', ''))\n",
    "    df1 = df1.withColumn(col, df1[col].substr(0, 64))\n",
    "    return df1\n",
    "\n",
    "# munge block dataframe\n",
    "def hex_to_int(x):\n",
    "    return int(x,16)\n",
    "\n",
    "def munge_block(df1):\n",
    "    df1 = explode_block(df1,'transaction_list')\n",
    "    udf_hex_to_int = udf(hex_to_int,IntegerType())\n",
    "    df1 = df1.withColumn('difficulty',udf_hex_to_int('difficulty'))\n",
    "    return df1\n",
    "\n",
    "\n",
    "# MAKE LIST OF TIER 1 MINERS\n",
    "def make_tier1_miner_list(df,threshold_tx_paid_out=10,threshold_blocks_mined_per_day=2.5):\n",
    "    # find all miners in period and make list\n",
    "    miner_list = [i.miner_address for i in df.select('miner_address').distinct().collect()]\n",
    "    # Count transactions paid out per day: group transactions by date and miner\n",
    "    # tier 1 = percentage mined per day > threshold || transactions paid out > threshold per day# make unique list of tier 1\n",
    "    df_temp = df.groupby('from_addr','block_timestamp').agg({'to_addr':'count'})\n",
    "    df_temp = df_temp.dropna()\n",
    "    # find daily mean\n",
    "    df_temp = df_temp.groupby('from_addr').agg({'count(to_addr)':'mean'})\n",
    "    df_temp = df_temp.filter(df_temp['avg(count(to_addr))']>=threshold_tx_paid_out)\n",
    "    # make list of tier 1 using tx paid out\n",
    "    list_a = [i.from_addr for i in df_temp.select('from_addr').distinct().collect()]\n",
    "    # check against miner list to ensure that only miners are included\n",
    "    list_a = list(set(miner_list) & set(list_a))\n",
    "    df_temp.unpersist()\n",
    "    \n",
    "    # Get percentage blocks mined per day: group by miner address, day and count\n",
    "    df_temp = df.groupby('miner_address','block_timestamp')\\\n",
    "        .agg({'block_number':'count'})\\\n",
    "        .withColumn('percent',100*(col('count(block_number)')/\n",
    "                                   sum(col('count(block_number)')).over(Window.partitionBy())))\n",
    "    df_temp = df_temp.groupby('miner_address').agg({'percent':'mean'})\n",
    "    df_temp = df_temp.filter(df_temp['avg(percent)']>=threshold_blocks_mined_per_day)\n",
    "    list_b = [i.miner_address for i in df_temp.select('miner_address').distinct().collect()]\n",
    "    print(list_b)\n",
    "    #merge lists, drop duplicates\n",
    "    tier1_miner_list = list(set(list_a+list_b))\n",
    "    del list_a,list_b\n",
    "\n",
    "    #check this list again miner_address\n",
    "    gc.collect()\n",
    "    return tier1_miner_list\n",
    "\n",
    "# dateformat = 'yyyy-mm-dd 00:00:00'\n",
    "# CHANGE INDIVIDUAL DATE TO TIMESTAMP\n",
    "def date_to_timestamp(date):\n",
    "    return datetime.strptime(date, \"%Y-%m-%d %H:%M:%S\").timestamp()\n",
    "\n",
    "# CREATE TIMESTAMP COLUMN IN DATETYPE FORMAT GIVEN A SPARK DATAFRAME\n",
    "def timestamp_to_date(df,col):\n",
    "    return df.withColumn('block_timestamp', f.from_unixtime('block_timestamp').cast(DateType()))\n",
    "\n",
    "# TRUNCATE SPARK DATAFRAME GIVEN STRING DATES\n",
    "# dateformat = 'yyyy-mm-dd 00:00:00'\n",
    "def truncate_dataframe(df1,startdate,enddate):\n",
    "    # get a month of data prior to startdate\n",
    "    \n",
    "    startdate = date_to_timestamp(startdate)# get a month of data prior to startdate\n",
    "    startdate1 = startdate - ( 30 * 24 * 60 * 60)\n",
    "    enddate = date_to_timestamp(enddate)\n",
    "    if startdate > enddate:\n",
    "        startdate = enddate\n",
    "    df1 = df1.filter((f.col('block_timestamp') >= startdate1) & \n",
    "              (f.col('block_timestamp') <= enddate))\n",
    "    df1 = timestamp_to_date(df1,'block_timestamp')\n",
    "    return df1\n",
    "\n",
    "# UDF FUNCTIONS TO INCLUDE EXTERNAL \n",
    "class MyUDFs:\n",
    "    #DICTIONARY WHEN MATCHING POOLNAME WITH MINER ADDRESS    \n",
    "    def populate(self):\n",
    "        self.df_poolinfo = pd.read_csv('../data/poolinfo.csv')\n",
    "        self.dict_poolinfo = dict(zip(self.df_poolinfo.address,self.df_poolinfo.poolname))\n",
    "        self.pool_keys = list(self.dict_poolinfo.keys())\n",
    "        \n",
    "    def get_poolname_label(self):\n",
    "        def ab(miner_address,pool_tier):\n",
    "            if miner_address in self.pool_keys:\n",
    "                return self.dict_poolinfo[miner_address]\n",
    "            else:\n",
    "                if pool_tier == 1:\n",
    "                    return miner_address[0:10]\n",
    "                else:\n",
    "                    return 'tier 2'\n",
    "        return udf(ab,StringType())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD FROM DATABASE\n",
    "df_tx = spark.read.parquet('../data/transaction.parquet')  \n",
    "df_block = spark.read.parquet('../data/block.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Make data warehouse for period (start_date, enddate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ENTER INPUT DATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "startdate = '2018-09-01 00:00:00'\n",
    "enddate = '2018-09-07 00:00:00'\n",
    "analysis_period = startdate[0:10]+' to '+enddate[0:10]+': '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-135-cc1f8e81ad2d>(77)truncate_dataframe()\n",
      "-> return df1\n",
      "(Pdb) df1.select('block_timestamp').show()\n",
      "+---------------+\n",
      "|block_timestamp|\n",
      "+---------------+\n",
      "|     2018-08-02|\n",
      "|     2018-08-02|\n",
      "|     2018-08-02|\n",
      "|     2018-08-02|\n",
      "|     2018-08-02|\n",
      "|     2018-08-02|\n",
      "|     2018-08-02|\n",
      "|     2018-08-02|\n",
      "|     2018-08-02|\n",
      "|     2018-08-02|\n",
      "|     2018-08-02|\n",
      "|     2018-08-02|\n",
      "|     2018-08-02|\n",
      "|     2018-08-02|\n",
      "|     2018-08-02|\n",
      "|     2018-08-02|\n",
      "|     2018-08-02|\n",
      "|     2018-08-02|\n",
      "|     2018-08-02|\n",
      "|     2018-08-02|\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "(Pdb) exit\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-5c07ecdaab40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# truncate dataframes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_block_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtruncate_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_block\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstartdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menddate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__index_level_0__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_block_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmunge_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_block_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_tx_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtruncate_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_tx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstartdate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menddate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'block_timestamp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-135-cc1f8e81ad2d>\u001b[0m in \u001b[0;36mtruncate_dataframe\u001b[0;34m(df1, startdate, enddate)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimestamp_to_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'block_timestamp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# UDF FUNCTIONS TO INCLUDE EXTERNAL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-135-cc1f8e81ad2d>\u001b[0m in \u001b[0;36mtruncate_dataframe\u001b[0;34m(df1, startdate, enddate)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimestamp_to_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'block_timestamp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# UDF FUNCTIONS TO INCLUDE EXTERNAL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bokeh_aion_analytics/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bokeh_aion_analytics/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# truncate dataframes\n",
    "df_block_1 = truncate_dataframe(df_block,startdate, enddate).drop('__index_level_0__')\n",
    "df_block_1 = munge_block(df_block_1)\n",
    "\n",
    "df_tx_1 = truncate_dataframe(df_tx,startdate,enddate).drop('block_timestamp')\n",
    "#\n",
    "df_tx_1 = df_tx_1.drop('__index_level_0__')\n",
    "df = df_block_1.join(df_tx_1, df_block_1.transaction_list == \n",
    "                                    df_tx_1.transaction_hash,how='left').drop('transaction_list')\n",
    "df_tx.unpersist()\n",
    "df_tx_1.unpersist()\n",
    "df_block.unpersist()\n",
    "df_block_1.unpersist()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify tier 1 addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_tx_paid_out = 10\n",
    "threshold_blocks_mined_per_day = 2.5 # Percentage\n",
    "tier1_miner_list = make_tier1_miner_list(df)\n",
    "# ADD A COLUMN CALLED POOL_TIER: 1 , 2\n",
    "pool_tier_udf = f.udf(lambda miner_address: 1 if \n",
    "                      miner_address in tier1_miner_list else 2, IntegerType())\n",
    "df = df.withColumn('pool_tier',pool_tier_udf(df.miner_address))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myUDF  = MyUDFs()\n",
    "myUDF.populate() \n",
    "df = df.withColumn('poolname',myUDF.get_poolname_label()(df[\"miner_address\"],df[\"pool_tier\"]))               \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis & plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar graphs of Tier blocks mined over period "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_miners(df,startdate):\n",
    "    # only plot requested period\n",
    "    startdate = date_to_timestamp(startdate)\n",
    "    df_temp = df.filter((f.col('block_timestamp')>= startdate))\n",
    "    \n",
    "    df_temp = df_temp.groupby('poolname').agg({'block_number':'count'})\\\n",
    "        .withColumn('percent',100*(col('count(block_number)')/\n",
    "                                   sum(col('count(block_number)')).over(Window.partitionBy())))\n",
    "    # convert small group to pandas for plotting\n",
    "    df_temp = df_temp.toPandas().sort_values(by=['percent'],ascending=False)\n",
    "    # Leave out the datashade option to get the tooltip to work\n",
    "    \n",
    "    bar = df_temp.hvplot.bar('poolname', ['count(block_number)'], rot=90,\n",
    "                             subplots=True, shared_axes=False,\n",
    "                             width=800,height=400,\n",
    "                             title=analysis_period +'Miners, blockcount')\n",
    "    bar_perc = df_temp.hvplot.bar('poolname', ['percent'], rot=90,\n",
    "                                  subplots=True, shared_axes=False,\n",
    "                                  width=800,height=400,\n",
    "                                  title=analysis_period +'Miners by %')\n",
    "\n",
    "    \n",
    "    hover = HoverTool(tooltips=[\n",
    "        (\"blocks mined\", \"$count(block_number)\"),\n",
    "        (\"percentage\", \"$percent\")\n",
    "    ])\n",
    "\n",
    "    #plot.options(tools=[hover])\n",
    "    # display plot\n",
    "    hv.show(bar)\n",
    "    hv.show(bar_perc)\n",
    "    del df_temp\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_difficulty(df,startdate):\n",
    "    # only plot requested period\n",
    "    startdate = date_to_timestamp(startdate)\n",
    "    df_temp = df.filter((f.col('block_timestamp')>= startdate))\n",
    "\n",
    "    df_temp = df_temp.select('block_timestamp','difficulty')\n",
    "    #convert from string to int\n",
    "    df_temp = df_temp.toPandas().sort_values(by=['block_timestamp'])\n",
    "    line = df_temp.hvplot.line(x='block_timestamp',y='difficulty',rot=90,\n",
    "                               width=800,height=400,\n",
    "                               title=analysis_period +'Difficulty')\n",
    "    hv.show(line)\n",
    "    del df_temp\n",
    "    gc.collect()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Activity Miners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_active_miners(df,startdate):\n",
    "    # only plot requested period\n",
    "    startdate = date_to_timestamp(startdate)\n",
    "    df_temp = df.filter((f.col('block_timestamp')>= startdate))\n",
    "    df_temp = df_temp.groupby('poolname','block_timestamp').agg({'block_number':'count'})\n",
    "    df_temp = df_temp.toPandas().sort_values(by=['block_timestamp'])\n",
    "    lines = df_temp.hvplot.line(x='block_timestamp',y='count(block_number)',rot=90,\n",
    "                                by='poolname',width=800,height=600,\n",
    "                                title=analysis_period+'pools blocks mined daily')\n",
    "    hv.show(lines)\n",
    "    del df_temp\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DISPLAY PLOTS/DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_miners(df,startdate)\n",
    "plot_difficulty(df,startdate)\n",
    "plot_active_miners(df,startdate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "total = t1 - t0\n",
    "print('time elaped = {} mins'.format(total/60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "100px",
    "left": "74px",
    "top": "111.133px",
    "width": "226px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
